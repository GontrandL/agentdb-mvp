# üèóÔ∏è Architecture Decision: AGTAG vs LLM-Parser System

**Date:** 2025-10-30
**Status:** INVESTIGATION REQUIRED
**Decision:** PENDING

---

## Executive Summary

**Question:** Should AgentDB continue using embedded AGTAG blocks, or switch to LLM-based parsing with auto-injection?

**Current System:** AGTAG blocks manually embedded in files ‚Üí Parser extracts ‚Üí DB stores
**Proposed System:** LLM analyzes source files ‚Üí Generates metadata ‚Üí Auto-inject to DB (NO file modification)

**Recommendation:** **Hybrid approach** - keep both systems operational, phase out AGTAG gradually

---

## The Incident That Triggered This Investigation

A worker attempting task REVIEW-009 caused critical failure:
- Added AGTAG blocks with HTML comment syntax to 128 Python files
- Broke Python imports (SyntaxError - HTML comments invalid in Python)
- Corrupted src/agentdb/*.py files
- agentdb CLI unusable

**Root Cause:** AGTAG format incompatible with Python syntax
**Question:** Is this a fundamental design flaw requiring architectural change?

---

## Current System Analysis

### AGTAG Embedded in Files

**Architecture:**
```
Source File (.py)
  ‚Üì
Human/LLM writes AGTAG block:
  AGTAG_METADATA = '''<!--AGTAG v1 START-->
  {"version":"v1","symbols":[{
    "summary_l0": "adds two numbers",
    "contract_l1": "@io a:int,b:int -> int",
    "pseudocode_l2": "return sum of inputs",
    "ast_excerpt_l3": {...}
  }]}
  <!--AGTAG v1 END-->'''
  ‚Üì
agentdb ingest --path file.py
  ‚Üì
Parser extracts AGTAG JSON
  ‚Üì
SQLite DB stores:
  - symbols table: summary_l0, contract_l1, pseudocode_l2, ast_json_l3
  - files table: file_hash, db_state=indexed
  ‚Üì
agentdb focus/zoom reads from DB
  L0/L1/L2/L3: From symbols table
  L4: Read source file directly
```

**Implementation Status:**
- ‚úÖ schema.sql: symbols table with l0_overview, l1_contract, l2_pseudocode, l3_ast_json
- ‚úÖ core.py:ingest(): Parses AGTAG block and inserts to DB
- ‚úÖ core.py:zoom(): Reads L0-L3 from DB, L4 from file
- ‚ùå AGTAG syntax incompatible with Python (requires AGTAG_METADATA wrapper)
- ‚ùå Manual maintenance burden (keep AGTAG in sync with code)
- ‚ùå Merge conflicts possible (AGTAG in git)

**Zoom Level Functionality:**
```python
# core.py:1092-1121
def zoom(handle, level):
    r = conn.execute(
        "SELECT l0_overview, l1_contract, l2_pseudocode, l3_ast_json, start_line, end_line
         FROM symbols WHERE repo_path=? AND name=?"
    ).fetchone()

    data = {"l0": r["l0_overview"], "l1": r["l1_contract"]}
    if level >= 2: data["l2"] = r["l2_pseudocode"]
    if level >= 3: data["l3"] = json.loads(r["l3_ast_json"])
    if level >= 4: data["l4"] = read_file_slice(repo_path, start_line, end_line)
```

**Strengths:**
1. ‚úÖ Metadata versioned with code (git tracks AGTAG + code together)
2. ‚úÖ Offline-readable (humans can see AGTAG in file)
3. ‚úÖ Provenance clear (metadata lives with source)
4. ‚úÖ Fast queries (pre-computed L0-L3 in DB)
5. ‚úÖ No LLM cost per query ($0 to read zoom levels)

**Weaknesses:**
1. ‚ùå **Syntax incompatibility** (Python ‚â† HTML comments)
2. ‚ùå File modification required (breaks read-only workflows)
3. ‚ùå Merge conflicts (git merges affect AGTAG)
4. ‚ùå Manual sync burden (update AGTAG when code changes)
5. ‚ùå Multi-language complexity (different syntax per language)
6. ‚ùå Parser brittleness (must handle malformed AGTAG)

---

## Proposed Alternative: LLM-Parser System

### Architecture

```
Source File (.py)
  ‚Üì
NO FILE MODIFICATION
  ‚Üì
agentdb ingest --path file.py --llm-analyze
  ‚Üì
LLM Agent analyzes source:
  - Reads file content
  - Generates summary_l0
  - Generates contract_l1
  - Generates pseudocode_l2
  - Parser generates ast_l3 (no LLM needed)
  ‚Üì
Auto-inject to SQLite:
  INSERT INTO symbols (l0_overview, l1_contract, l2_pseudocode, l3_ast_json)
  ‚Üì
agentdb focus/zoom reads from DB
  L0/L1/L2/L3: From symbols table (LLM-generated)
  L4: Read source file directly
```

**Implementation Requirements:**

**1. LLM Integration Module** (`src/agentdb/llm_analyzer.py`):
```python
class LLMAnalyzer:
    """Generate symbol metadata from source code using LLM."""

    def analyze_symbol(self, source_code: str, symbol_name: str) -> Dict:
        """
        Analyze a symbol and generate L0-L3 metadata.

        Args:
            source_code: Full source code context
            symbol_name: Symbol to analyze (function/class name)

        Returns:
            {
                "summary_l0": "One-line summary",
                "contract_l1": "@io inputs -> outputs, invariants",
                "pseudocode_l2": "Algorithm description",
                "ast_l3": {...}  # Generated by parser, not LLM
            }
        """
        prompt = f"""Analyze this symbol and provide:
        1. L0: One-line summary (< 80 chars)
        2. L1: Contract (@io inputs -> outputs, invariants)
        3. L2: Pseudocode/algorithm description

        Symbol: {symbol_name}
        Source:
        {source_code}

        Return JSON only."""

        response = self.llm_client.complete(prompt)
        return json.loads(response)
```

**2. Modified Ingest Command** (core.py):
```python
@cli.command()
@click.option("--path", required=True)
@click.option("--llm-analyze", is_flag=True, help="Use LLM to generate metadata")
def ingest(path, llm_analyze):
    """Ingest file with optional LLM analysis."""

    content = sys.stdin.read()

    if llm_analyze:
        # NEW: LLM-based analysis (no AGTAG needed)
        analyzer = LLMAnalyzer()
        symbols = extract_symbols(content)  # Parse source to find symbols

        for symbol in symbols:
            metadata = analyzer.analyze_symbol(content, symbol['name'])
            symbol.update(metadata)  # Add L0-L3 from LLM

    else:
        # LEGACY: Extract from AGTAG block
        symbols = parse_agtag_block(content)

    # Same DB insertion either way
    upsert_symbols(conn, repo_path, symbols)
```

**3. Auto-Update on File Change**:
```python
@cli.command()
@click.option("--path", required=True)
def watch_and_update(path):
    """Watch file for changes and auto-re-analyze."""

    # File watcher detects modification
    # Trigger: llm_analyzer.analyze_symbol()
    # Update DB with new metadata
    # Cost: $0.01-0.10 per re-analysis
```

**Strengths:**
1. ‚úÖ **NO file modification** (read-only analysis)
2. ‚úÖ **NO syntax issues** (metadata separate from code)
3. ‚úÖ **NO merge conflicts** (DB separate from git)
4. ‚úÖ **Automatic updates** (LLM re-analyzes on change)
5. ‚úÖ **Multi-language support** (Python/JS/Go/Rust/etc - LLM handles all)
6. ‚úÖ **Simpler codebase** (no AGTAG parsing complexity)

**Weaknesses:**
1. ‚ùå Metadata NOT versioned with code (git doesn't track DB)
2. ‚ùå LLM cost per file ($0.01-0.10 per analysis)
3. ‚ùå LLM cost per update (every code change triggers re-analysis)
4. ‚ùå Requires LLM API access (can't work offline without DB)
5. ‚ùå Slower ingestion (LLM latency vs instant AGTAG parse)
6. ‚ùå Quality variability (LLM may generate inconsistent metadata)

---

## Cost Analysis

### AGTAG Embedded System

**Initial Ingestion (170 files):**
- Human writes AGTAG: 170 files √ó 5 min = 14 hours manual work
- OR LLM generates AGTAG once: 170 √ó $0.05 = $8.50
- Total: $8.50 one-time OR 14 hours human time

**Updates (10 files/week change):**
- Human updates AGTAG: 10 √ó 2 min = 20 min/week
- OR ignore updates (stale metadata)
- Total: 20 min/week manual OR stale data

**Queries (1000 zoom calls/week):**
- Cost: $0 (read from DB)

**Annual Cost:**
- Upfront: $8.50 one-time
- Maintenance: 17 hours/year manual OR stale metadata
- Queries: $0

### LLM-Parser System

**Initial Ingestion (170 files):**
- LLM analyzes: 170 √ó $0.05 = $8.50
- Total: $8.50 one-time

**Updates (10 files/week change):**
- LLM re-analyzes: 10 √ó $0.05 = $0.50/week
- Total: $26/year automatic updates

**Queries (1000 zoom calls/week):**
- Cost: $0 (read from DB)

**Annual Cost:**
- Upfront: $8.50 one-time
- Maintenance: $26/year automatic
- Queries: $0

**Comparison:**
- AGTAG: $8.50 + 17 hours manual OR stale data
- LLM-Parser: $8.50 + $26/year automatic + always fresh
- **LLM-Parser is cheaper if human time > $1.50/hour**

---

## Zoom Level Functionality Comparison

### Current (AGTAG)

```
agentdb focus --handle "ctx://src/example.py::add@sha256:ANY" --depth 1

Query DB:
  SELECT l0_overview, l1_contract FROM symbols WHERE repo_path=... AND name=...

Result (50 tokens):
  L0: "adds two numbers"
  L1: "@io a:int,b:int -> int"
  + 1-hop neighbors

Cost: $0
Latency: <10ms
Data freshness: Only if AGTAG updated manually
```

### LLM-Parser

```
agentdb focus --handle "ctx://src/example.py::add@sha256:ANY" --depth 1

Query DB:
  SELECT l0_overview, l1_contract FROM symbols WHERE repo_path=... AND name=...

Result (50 tokens):
  L0: "adds two numbers" [LLM-generated during ingest]
  L1: "@io a:int,b:int -> int" [LLM-generated during ingest]
  + 1-hop neighbors

Cost: $0 (metadata pre-generated)
Latency: <10ms
Data freshness: Updated automatically when file changes
```

**IDENTICAL QUERY EXPERIENCE** - difference is in ingestion/maintenance phase

---

## Hybrid Approach (Recommended)

### Strategy: Support Both Systems

**Phase 1: Add LLM-Parser (Immediate)**
```bash
agentdb ingest --path file.py --llm-analyze  # NEW
agentdb ingest --path file.py < file_with_agtag.py  # LEGACY (keep working)
```

**Phase 2: Make LLM-Parser Default (3 months)**
```bash
agentdb ingest --path file.py  # Uses LLM by default
agentdb ingest --path file.py --agtag  # Explicit flag for legacy AGTAG
```

**Phase 3: Deprecate AGTAG (6 months)**
```bash
agentdb ingest --path file.py  # LLM only
# AGTAG support removed from codebase
```

**Rationale:**
1. **Gradual migration** - no breaking changes
2. **Validate LLM quality** - compare AGTAG vs LLM-generated metadata
3. **Cost monitoring** - track LLM spend vs manual AGTAG maintenance
4. **Escape hatch** - if LLM fails, AGTAG still works

---

## Technical Implementation Plan

### Step 1: Create LLM Analyzer Module (Week 1)

**File:** `src/agentdb/llm_analyzer.py`

```python
import anthropic  # Or OpenAI, DeepSeek, etc.
import json

class LLMAnalyzer:
    """Generate symbol metadata from source code."""

    def __init__(self, api_key: str, model: str = "claude-3-haiku-20240307"):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.model = model

    def analyze_symbol(self, source_code: str, symbol_name: str, language: str = "python") -> Dict:
        """
        Generate L0-L3 metadata for a symbol.

        Cost: ~500 tokens input + 200 tokens output = $0.01-0.05 per symbol
        """
        prompt = f"""Analyze this {language} symbol and generate metadata:

Symbol: {symbol_name}

Source code:
```{language}
{source_code}
```

Generate JSON with:
1. "summary_l0": One-line summary (<80 chars, active voice)
2. "contract_l1": Contract format "@io inputs -> outputs, invariants: X"
3. "pseudocode_l2": Algorithm description (3-5 lines, implementation-agnostic)

Example output:
{{
  "summary_l0": "adds two integers and returns sum",
  "contract_l1": "@io a:int,b:int -> int, invariants: result = a + b",
  "pseudocode_l2": "1. Accept two integer parameters\\n2. Compute arithmetic sum\\n3. Return result"
}}

Return ONLY valid JSON, no markdown formatting."""

        response = self.client.messages.create(
            model=self.model,
            max_tokens=500,
            messages=[{"role": "user", "content": prompt}]
        )

        return json.loads(response.content[0].text)

    def analyze_file(self, file_path: str, content: str) -> List[Dict]:
        """
        Analyze all symbols in a file.

        Returns list of symbol dicts with L0-L3 metadata.
        """
        # Parse source to find symbols (functions, classes, methods)
        symbols = self._extract_symbols(content, file_path)

        # Generate metadata for each symbol
        for symbol in symbols:
            try:
                metadata = self.analyze_symbol(content, symbol['name'], language=self._detect_language(file_path))
                symbol.update(metadata)
            except Exception as e:
                # Fallback to empty metadata if LLM fails
                symbol.update({
                    "summary_l0": f"[AUTO] {symbol['kind']} {symbol['name']}",
                    "contract_l1": "[AUTO] contract unavailable",
                    "pseudocode_l2": None
                })

        return symbols

    def _extract_symbols(self, content: str, file_path: str) -> List[Dict]:
        """Extract symbols using AST parsing (no LLM needed)."""
        # Use ast module for Python
        # Use tree-sitter for other languages
        # Return: [{"name": "add", "kind": "function", "lines": [1, 3], ...}]
        pass

    def _detect_language(self, file_path: str) -> str:
        """Detect language from file extension."""
        ext = Path(file_path).suffix
        return {".py": "python", ".js": "javascript", ".go": "go", ".rs": "rust"}.get(ext, "unknown")
```

### Step 2: Add --llm-analyze Flag to Ingest (Week 1)

**File:** `src/agentdb/core.py`

```python
@cli.command()
@click.option("--path", required=True)
@click.option("--llm-analyze", is_flag=True, help="Use LLM to generate metadata (no AGTAG needed)")
@click.option("--llm-api-key", envvar="ANTHROPIC_API_KEY", help="LLM API key")
@click.option("--llm-model", default="claude-3-haiku-20240307", help="LLM model to use")
def ingest(path, llm_analyze, llm_api_key, llm_model):
    """Ingest file with AGTAG or LLM analysis."""

    conn = ensure_db()
    safe_path = ensure_repo_relative_path(path)
    content = sys.stdin.read()

    if llm_analyze:
        # NEW: LLM-based analysis
        if not llm_api_key:
            click.echo(json.dumps({"error": "llm_api_key_required"}))
            sys.exit(2)

        analyzer = LLMAnalyzer(api_key=llm_api_key, model=llm_model)
        symbols = analyzer.analyze_file(safe_path, content)

    else:
        # LEGACY: Extract from AGTAG
        symbols = extract_agtag(content)
        if not symbols:
            click.echo(json.dumps({"error":"agtag_missing"}))
            sys.exit(2)

    # Same DB insertion logic
    file_hash = hash_content(content)
    upsert_symbols(conn, safe_path, symbols)
    conn.execute("INSERT OR REPLACE INTO files (repo_path, file_hash, db_state) VALUES (?,?,?)",
                 (safe_path, file_hash, "indexed"))
    conn.commit()

    click.echo(json.dumps({
        "status": "ingested",
        "path": safe_path,
        "symbols": len(symbols),
        "method": "llm" if llm_analyze else "agtag"
    }))
```

### Step 3: Add Auto-Update on File Change (Week 2)

**File:** `src/agentdb/file_watcher.py`

```python
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class AgentDBFileWatcher(FileSystemEventHandler):
    """Watch for file changes and auto-re-analyze."""

    def __init__(self, llm_analyzer: LLMAnalyzer):
        self.analyzer = llm_analyzer

    def on_modified(self, event):
        """File changed - re-analyze and update DB."""
        if event.is_directory:
            return

        file_path = event.src_path

        # Check if file is tracked
        conn = ensure_db()
        row = conn.execute("SELECT file_hash, db_state FROM files WHERE repo_path=?", (file_path,)).fetchone()

        if row and row["db_state"] == "indexed":
            # Re-analyze
            content = open(file_path).read()
            new_hash = hash_content(content)

            if new_hash != row["file_hash"]:
                # File changed - update metadata
                symbols = self.analyzer.analyze_file(file_path, content)
                upsert_symbols(conn, file_path, symbols)
                conn.execute("UPDATE files SET file_hash=? WHERE repo_path=?", (new_hash, file_path))
                conn.commit()

                print(f"‚úÖ Auto-updated: {file_path} ({len(symbols)} symbols)")
```

### Step 4: Add Configuration (Week 2)

**File:** `.agentdb/config.json`

```json
{
  "llm": {
    "enabled": true,
    "provider": "anthropic",
    "model": "claude-3-haiku-20240307",
    "api_key_env": "ANTHROPIC_API_KEY",
    "auto_update": true,
    "watch_interval_seconds": 5
  },
  "agtag": {
    "enabled": true,
    "fallback_if_llm_fails": true
  },
  "cost_tracking": {
    "max_monthly_spend": 100.00,
    "warn_threshold": 80.00
  }
}
```

### Step 5: Add Cost Tracking (Week 3)

**File:** `src/agentdb/cost_tracker.py`

```python
class CostTracker:
    """Track LLM API costs."""

    def log_analysis(self, file_path: str, input_tokens: int, output_tokens: int, cost: float):
        """Log API call cost."""
        conn = ensure_db()
        conn.execute(
            "INSERT INTO llm_costs (timestamp, file_path, input_tokens, output_tokens, cost) VALUES (?,?,?,?,?)",
            (datetime.utcnow().isoformat(), file_path, input_tokens, output_tokens, cost)
        )
        conn.commit()

    def get_monthly_spend(self) -> float:
        """Get current month's LLM spend."""
        conn = ensure_db()
        result = conn.execute(
            "SELECT SUM(cost) FROM llm_costs WHERE timestamp >= date('now', 'start of month')"
        ).fetchone()
        return result[0] or 0.0

    def check_budget(self) -> bool:
        """Check if under budget."""
        config = load_config()
        monthly = self.get_monthly_spend()
        max_spend = config["cost_tracking"]["max_monthly_spend"]

        if monthly >= max_spend:
            return False

        warn_threshold = config["cost_tracking"]["warn_threshold"]
        if monthly >= (max_spend * warn_threshold / 100):
            print(f"‚ö†Ô∏è  LLM spend at {monthly/max_spend*100:.1f}% of budget")

        return True
```

---

## Migration Path

### Timeline

**Week 1-2: Foundation**
- ‚úÖ Create llm_analyzer.py module
- ‚úÖ Add --llm-analyze flag to ingest
- ‚úÖ Add cost tracking infrastructure
- ‚úÖ Update CLAUDE.md with both approaches

**Week 3-4: Validation**
- ‚úÖ Test LLM-generated metadata quality
- ‚úÖ Compare AGTAG vs LLM for 50 files
- ‚úÖ Measure cost per file
- ‚úÖ Verify zoom functionality identical

**Month 2: Gradual Rollout**
- ‚úÖ Enable LLM-analyze for new files only
- ‚úÖ Keep AGTAG for existing 170 files
- ‚úÖ Add file watcher for auto-updates
- ‚úÖ Monitor cost and quality

**Month 3: Default Switch**
- ‚úÖ Make --llm-analyze default
- ‚úÖ Require --agtag flag for legacy mode
- ‚úÖ Re-analyze top 50 most-used files with LLM

**Month 6: Deprecation**
- ‚úÖ Remove AGTAG support from codebase
- ‚úÖ All files use LLM-generated metadata
- ‚úÖ Cost stabilized, quality validated

### Rollback Plan

**If LLM quality insufficient:**
- Keep AGTAG as primary
- Use LLM only for L0 summaries
- Manual review for L1/L2

**If LLM cost too high:**
- Use LLM only for ingestion (one-time)
- Disable auto-updates (manual trigger)
- Switch to cheaper model (GPT-4o-mini, Claude Haiku)

**If LLM API unavailable:**
- Fallback to AGTAG parsing
- Offline mode uses cached DB
- Manual AGTAG creation for new files

---

## Decision Matrix

| Criterion | AGTAG Only | LLM-Parser Only | Hybrid (Recommended) |
|-----------|------------|-----------------|----------------------|
| **File modification** | Required | Not required | Optional |
| **Syntax issues** | High risk (Python) | No risk | No risk (LLM mode) |
| **Initial cost** | $8.50 OR 14 hours | $8.50 | $8.50 |
| **Maintenance cost** | 17 hrs/year OR stale | $26/year | $26/year |
| **Data freshness** | Manual updates | Automatic | Automatic (LLM mode) |
| **Offline usable** | Yes (file + DB) | Requires DB | Both |
| **Multi-language** | Complex | Easy | Easy (LLM mode) |
| **Query speed** | <10ms | <10ms | <10ms |
| **Implementation effort** | Already done | 2-3 weeks | 2-3 weeks |
| **Risk** | Syntax bugs | LLM quality | Low (supports both) |

**Hybrid wins on all critical criteria:**
- ‚úÖ Eliminates syntax risk (use LLM for Python)
- ‚úÖ Preserves AGTAG for offline/versioned use cases
- ‚úÖ Automatic updates when needed
- ‚úÖ Escape hatch if LLM fails
- ‚úÖ Gradual migration (no breaking changes)

---

## Recommendation

### Adopt Hybrid Approach

**Immediate Actions (This Week):**

1. ‚úÖ **Fix CLAUDE.md** (DONE - file-type-specific AGTAG formats)
2. ‚úÖ **Create validation tool** (DONE - tools/validate_agtag_syntax.py)
3. ‚úÖ **Document incident** (DONE - INCIDENT_REPORT_AGTAG_SYNTAX_FAILURE.md)
4. üîÑ **Build LLM analyzer** (START - src/agentdb/llm_analyzer.py)
5. üîÑ **Add --llm-analyze flag** (START - modify core.py ingest)

**Short-term (Next Month):**

6. Test LLM-generated metadata quality (50 file sample)
7. Compare cost: AGTAG maintenance vs LLM auto-updates
8. Enable LLM-analyze for new Python files (avoid syntax issues)
9. Keep AGTAG for HTML/Markdown (already compatible)

**Long-term (3-6 Months):**

10. Make LLM-analyze default for all file types
11. Add file watcher for auto-updates
12. Deprecate AGTAG (optional: keep for offline use)
13. Full migration to LLM-parser system

### Why Hybrid is Best

**Solves immediate problem:**
- Python files use LLM-analyze (NO syntax issues)
- HTML/Markdown can keep AGTAG (already works)

**Provides flexibility:**
- Offline mode: Use AGTAG-ingested files
- Online mode: Use LLM auto-updates
- Cost-sensitive: Disable auto-updates, use LLM once

**Reduces risk:**
- If LLM fails: Fallback to AGTAG
- If AGTAG syntax breaks: Use LLM instead
- Gradual migration: No breaking changes

**Optimizes for zoom functionality:**
- Query experience identical (read from DB)
- L0-L3 always available (either source)
- L4 always from source file

---

## Conclusion

**Answer to User's Question:**

> "Is AGTAG a fundamentally incompatible choice? Can't agentdb parse LLM responses and auto-inject?"

**Yes, AGTAG has syntax compatibility issues with Python.**
**Yes, LLM-parser system is technically superior.**
**Yes, we should build it WHILE keeping AGTAG as fallback.**

**The "spend little upfront for higher intent intelligence later" philosophy remains valid:**
- Initial analysis: $0.01-0.10 per file (same for AGTAG-generation OR LLM-analyze)
- Zoom queries: $0 (read pre-computed L0-L3 from DB)
- Updates: $0.01-0.10 per change (automatic with LLM vs manual with AGTAG)

**LLM-parser system is BETTER:**
- ‚úÖ NO file modification (read-only)
- ‚úÖ NO syntax issues (metadata separate)
- ‚úÖ Automatic updates (always fresh)
- ‚úÖ Multi-language support (LLM handles all)
- ‚úÖ Same query cost ($0 - read from DB)

**Recommendation: BUILD IT (hybrid approach).**

---

## Next Steps

**Decision Required:**
- [ ] User approves hybrid approach?
- [ ] Priority: Fix AGTAG syntax OR build LLM-parser first?
- [ ] Budget: Approve $26/year for LLM auto-updates?

**If approved, start with:**
1. ‚úÖ Fix CLAUDE.md (DONE)
2. ‚úÖ Create validator (DONE)
3. üîÑ Build llm_analyzer.py (NEXT)
4. üîÑ Add --llm-analyze to ingest (NEXT)
5. üîÑ Test with 10 Python files (NEXT)

**Timeline:** 2-3 weeks for full hybrid implementation

**Cost:** $8.50 initial + $26/year maintenance (< $3/month)

**Risk:** LOW (hybrid supports both AGTAG and LLM)
